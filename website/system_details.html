<!doctype html>
<html lang="en">

<!-- HEADER: Title, Stylesheet -->
<head>
  <meta charset="utf-8">
  <title>Soundpound</title>
  <link rel="stylesheet" href="http://yui.yahooapis.com/pure/0.5.0/pure-min.css">
  <link rel="stylesheet" href="misc.css">
  <h1 align="center">Soundpound</h1>
</head>

<!-- NAVIGATION -->
<div class="pure-menu pure-menu-open pure-menu-horizontal" align="center">
    <ul>
        <li><a href="home.html">Overview</a></li>
        <li><a href="results.html">Results</a></li>
        <li class="pure-menu-selected"><a href="system_details.html">System Details</a></li>
    </ul>
    <hr>
</div>

<!-- BODY -->
<body>

<!-- INTRO -->
<h4 align="center">Introduction</h4>

<p> Soundpound is a data-driven approach to inferring which sounds should be played by an air-drummer. It was completed as the course project for <a href="http://cs.brown.edu/courses/csci2951-b/">CSCI 2951-B in Fall 2014</a>. The system takes as input a video of a person playing the air-drums, and returns as output the same video annotated with drum sounds that are believed to be realistic. The key observation is that small segments of movement of an air drummer ought to appear similar to those of a real drummer.
  </p>

<!-- PIPELINE MAIN STEPS -->
<p>
The full system has four main steps:
<ol class="indentlist">
  <li> Apply optical flow to input video, perform pooling on resulting vector field
  <li> Group frames into sequences of frames (referred to as segments henceforth)
  <li> Compute Nearest Neighbor for each of the input video's segments
  <li> Stitch the sound together for each segment.
</ol>
</p>

<!-- PIPELINE IMAGE -->
<p>
The full pipeline is visualized as follows:

<img src="images/pipeline.jpg" alt="Full Pipeline" style="width:560px;">

Soundpound takes around two times longer two run than the length of the input video (i.e. for a 10 second video, the system will take around 20 seconds). The primary computational bottleneck is running optical flow and pooling. The dataset is all serialized in its featurized form, so it is extremely quick to load and search through.
</p>

<!-- DATA SET -->
<h4 align="center">Data Set</h4>

<p>The ENST-Drums data set was kindly provided by Olivier Gillet and Gael Richard. The full data set may be requested from them <a href="http://www.tsi.telecom-paristech.fr/aao/en/2010/02/19/enst-drums-an-extensive-audio-visual-database-for-drum-signals-processing/">here</a>. It includes approximately an hour of footage of three different drummers playing three different (and extremely varied) drum kits - some of the kits include peculiar percussive instruments such as cowbells, and others are equipped with extra toms and cymbals. The drummers are recorded playing each percussiion instrument with a few different sticks (e.g. mallets, brushes, etc.) from two different camera angles. For experimentation, the full dataset was not used due to the complexity of some of the sequences (i.e. fills, phrases). Here are some example images from the dataset:
  </p>

<!-- IMAGES OF DATASET -->
<div class="container">
    <img src="images/drummer1.png" alt="Drummer 1" style="width:120px; float: left; margin-right:3%">
  <img src="images/drummer2.png" alt="Drummer 2" style="width:120px;float: left;margin-right:3%">
  <img src="images/drummer3.png" alt="Drummer 3" style="width:120px;float: left;margin-right:3%">
  <img src="images/drummer4.png" alt="Drummer 4" style="width:120px;float: left;margin-right:3%">
</div>

<!-- RELATED WORK -->
<h4 align="center">Related Work</h4>

<p>
  </p>

<!-- REPRESENTATION -->
<h4 align="center">Representation</h4>

<p> The crux of the representation for each video consists of a vector field that results from applying Optical Flow to each pair of frames in the video. Optical flow boils down to computing a dense grid of keypoints for each frame and tracking the movement of those keypoints to the next frame. Thus, each frame is represented as a vector field representing how much each keypoint moved from the previous frame. The implementation that I used was the approach introduced by Gunnar Farneback in <a href="http://www.diva-portal.org/smash/get/diva2:273847/FULLTEXT01.pdf">Image Analysis, 2003</a>. For each frame, I performed a pooling operation across the vector field, reducing the representation for each frame down to a single integer value. Frames were then grouped into segments of N consecutive frames (results reported with N=25). This allows for the representation to capture changes in movement, such as downward motion followed immediately by abrupt backward motion. This was critical for achieving good results (with N=1 the Mean Temporal Error was effectively the same as random).
  </p>

<!-- Pooling -->
  <h4 align="center">Pooling</h4>

<p> The different pooling functions considered were:
</p>

  <ol>
    <li style="margin-bottom: 9%"> Average magnitude of Optical Flow vectors:

    <img src="images/avg_pooling.jpg" alt="Average Pooling" style="width:400px; margin-top: 2%">

    <li style="margin-bottom: 9%"> Max magnitude of Optical Flow vectors

  <img src="images/max_pooling.jpg" alt="Max Pooling" style="width:415px; margin-top: 2%">

    <li style="margin-bottom: 9%"> Sum of the magnitudes of Optical Flow vectors
      
  <img src="images/sum_pooling.jpg" alt="Sum Pooling" style="width:360px; margin-top: 2%">

    <li style="margin-bottom: 9%"> Angle of the maximum amplitude optical flow vector:

  <img src="images/pool_max_angle_1.jpg" alt="Max Angle Pooling" style="width:295px; margin-top: 2%">
  <p>
    Where:
  </p>
  <img src="images/pool_max_angle_2.jpg" alt="Max Angle Pooling" style="width:480px; margin-top: 2%">

    <li style="margin-bottom: 9%"> Weighted average of Optical Flow angles. (weighted by magnitude).

<img src="images/weighted_vec_pooling.jpg" alt="Max Angle Pooling" style="width:540px; margin-top: 2%">

  </ol>

  <p>I processed a small subset of the full data into serialized feature representations using each of the above features, and compared the Mean Temporal Error on a held out set of data. This involved computing the inferred sound by running the full Soundpound system and compting the Error between the ground truth sound for the given video and the audio inferred by Soundpound.
  </p>


<h4 align="center">Evaluation</h4>

<p>
As stated, this problem is fundamentally difficult to quantitatively evaluate. In order to evaluate the system, I rephrased the problem in terms of reidentifying the correct sound for an arbitrary video in the dataset. This allowed me to treat it like a standard machine learning problem, and break off a small chunk of the dataset for use as a test set. Then, using Soundpound, I approximated the correct drumming audio for a given input video and compared it to the ground truth audio. Unfortunately I was unable to find any existing distance metrics that were standard for audio (or sequences of integers, for that matter). I introduce the Mean Temporal Error metric for determining the distance between two sound waves:
</p>

<img src="images/mte.jpg" alt="Mean Temporal Error" style="width:480px;">

<p> The MTE effectively penalizes cases where the inferred sound incorrectly plays a drum sound, or incorrectly misses a drum sound. The setting of the parameter K is significant, then, as it is sensitive to the volume of the recording. Fortunately for this dataset, it was easy to pick a K (50) that achieved the desired cost function. Using the MTE, I was able to quantitatively evaluate the performance of my system:</p>

<img src="images/evaluation.jpg" alt="Evaluation Framework" style="width:480px;">



<!-- NEAREST NEIGHBORS -->
<h4 align="center">Finding Neighbors</h4>

<p>
I serialized the entire data set of videos in their featurized format. That is, I computed the Optical Flow of each video in the dataset, performed a pooling operation on the vector field, and grouped the frames into segments. Each segment was then stored in the dataset as its own entity (i.e. a single video was stored as several N-frame sequences). For the input video, the same processing was applied (Optical Flow + Pooling). Then, a simple nearest neighbor search for each segment identified the portions of other videos whose motions corresponded well to the given input segment's motion. The resulting sound is simply the stitched together sound of each matched segment.
</p>


<h4 align="center">Quantitative Results</h4>

<p>
</p>

<img src="images/results_feature_type.jpg" alt="Comparison of Features" style="width:480px;">
<img src="images/results_data_size.jpg" alt="Comparison of Dataset Size" style="width:480px;">

<h4 align="center">Conclusion</h4>
<p> The problem of inferring the correct sound is somewhat unconstrained, and is perhaps not well suited to data driven approaches. Some <a href="results.html">results</a> are encouraging, though I suspect there are pure algorithmic approaches that could out-perform what I have put together here. Additinally, I discovered that there is a noticeable lack of distance metrics for time-series data. Much effort has been dedicated to distance metrics in language (i.e. Edit distance, <a href="http://en.wikipedia.org/wiki/Word_ladder">Lewis Carrol's Word Ladder</a>), as one might expect.
</p>

</body>

</html>